{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:c7c8ga3a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce88d1a0f7944d4e83eb01a2dddea68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▇▁█▇▁█▇▁</td></tr><tr><td>f1_0</td><td>█▇▁█▇▁█▇▁</td></tr><tr><td>f1_1</td><td>▁▂█▁▂█▁▂█</td></tr><tr><td>max_length</td><td>▁▁▁▃▃▃███</td></tr><tr><td>precision_0</td><td>▁▂█▁▂█▁▂█</td></tr><tr><td>precision_1</td><td>█▅▁█▅▁█▅▁</td></tr><tr><td>recall_0</td><td>█▇▁█▇▁█▇▁</td></tr><tr><td>recall_1</td><td>▁▂█▁▂█▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.74413</td></tr><tr><td>f1_0</td><td>0.84644</td></tr><tr><td>f1_1</td><td>0.23332</td></tr><tr><td>hidden_sizes</td><td>[512, 256, 128]</td></tr><tr><td>max_length</td><td>512</td></tr><tr><td>model_name</td><td>distilbert-base-unca...</td></tr><tr><td>precision_0</td><td>0.80872</td></tr><tr><td>precision_1</td><td>0.30414</td></tr><tr><td>recall_0</td><td>0.88785</td></tr><tr><td>recall_1</td><td>0.18925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">single_feature_text_model_title</strong> at: <a href='https://wandb.ai/highlander-rahat/campaign_success_prediction/runs/c7c8ga3a' target=\"_blank\">https://wandb.ai/highlander-rahat/campaign_success_prediction/runs/c7c8ga3a</a><br/> View project at: <a href='https://wandb.ai/highlander-rahat/campaign_success_prediction' target=\"_blank\">https://wandb.ai/highlander-rahat/campaign_success_prediction</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241120_121023-c7c8ga3a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:c7c8ga3a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rahatibnrafiq/research/research/gofundme/wandb/run-20241120_125454-dr6ja3sk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/highlander-rahat/campaign_success_prediction/runs/dr6ja3sk' target=\"_blank\">single_feature_text_model_title</a></strong> to <a href='https://wandb.ai/highlander-rahat/campaign_success_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/highlander-rahat/campaign_success_prediction' target=\"_blank\">https://wandb.ai/highlander-rahat/campaign_success_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/highlander-rahat/campaign_success_prediction/runs/dr6ja3sk' target=\"_blank\">https://wandb.ai/highlander-rahat/campaign_success_prediction/runs/dr6ja3sk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/highlander-rahat/campaign_success_prediction/runs/dr6ja3sk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2d35a7850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from transformers import DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AlbertTokenizer, DistilBertModel, BertModel, RobertaModel, AlbertModel\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"campaign_success_prediction\", name=\"single_feature_text_model_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "models = [\n",
    "    (\"distilbert-base-uncased\", DistilBertTokenizer, DistilBertModel),\n",
    "    (\"bert-base-uncased\", BertTokenizer, BertModel),\n",
    "    (\"roberta-base\", RobertaTokenizer, RobertaModel),\n",
    "    (\"albert-base-v2\", AlbertTokenizer, AlbertModel),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define configurations for neural network layers\n",
    "nn_configs = [\n",
    "    {\"hidden_sizes\": [128], \"activation\": \"relu\"},\n",
    "    {\"hidden_sizes\": [256, 128], \"activation\": \"relu\"},\n",
    "    {\"hidden_sizes\": [512, 256, 128], \"activation\": \"relu\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max lengths for tokenization\n",
    "max_lengths = [128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test, hidden_sizes, model_name, max_length):\n",
    "    layer_sizes = []\n",
    "    for i in range(len(hidden_sizes) - 1):\n",
    "        layer_sizes.append((hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "    nn_model = MLPClassifier(\n",
    "        hidden_layer_sizes=tuple(hidden_sizes),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    nn_model.fit(X_train, y_train)\n",
    "    y_pred = nn_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        \"model_name\": model_name,\n",
    "        \"hidden_sizes\": str(hidden_sizes),\n",
    "        \"max_length\": max_length,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_0\": precision[0],\n",
    "        \"recall_0\": recall[0],\n",
    "        \"precision_1\": precision[1],\n",
    "        \"recall_1\": recall[1],\n",
    "        \"f1_0\": f1[0],\n",
    "        \"f1_1\": f1[1],\n",
    "    })\n",
    "\n",
    "    print(f\"\\nModel: {model_name}, Hidden Sizes: {hidden_sizes}, Max Length: {max_length}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision (Class 0): {precision[0]}, Recall (Class 0): {recall[0]}\")\n",
    "    print(f\"Precision (Class 1): {precision[1]}, Recall (Class 1): {recall[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_path = 'dataset/analysis_campaign_ML_cleaned.json'\n",
    "# file_path = 'dataset/campaigns_for_analysis.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "filtered_data = [{'Title': campaign.get('Title', ''), 'Description': campaign.get('Description', ''), 'success': campaign.get('success', '')} for campaign in data]\n",
    "for campaign in filtered_data:\n",
    "    if 'success' in campaign:  # Ensure the key exists\n",
    "        campaign['success'] = 1 if campaign['success'].lower() == 'yes' else 0\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_data)\n",
    "train_data, test_data = train_test_split(filtered_df, test_size=0.3, random_state=42, stratify=filtered_df['success'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def encode_text(texts, tokenizer, encoder, max_length):\n",
    "    inputs = tokenizer(list(texts), padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments for distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahatibnrafiq/miniforge3/envs/nlp/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing with max_length=128...\n"
     ]
    }
   ],
   "source": [
    "for model_name, tokenizer_class, encoder_class in models:\n",
    "    print(f\"Running experiments for {model_name}...\")\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    encoder = encoder_class.from_pretrained(model_name)\n",
    "    for max_length in max_lengths[:2]:\n",
    "        print(f\"Tokenizing with max_length={max_length}...\")\n",
    "        title_embeddings_train = encode_text(train_data[\"Title\"], tokenizer, encoder, max_length)\n",
    "        title_embeddings_test = encode_text(test_data[\"Title\"], tokenizer, encoder, max_length)\n",
    "        \n",
    "        description_embeddings_train = encode_text(train_data[\"Description\"], tokenizer, encoder, max_length)\n",
    "        description_embeddings_test = encode_text(test_data[\"Description\"], tokenizer, encoder, max_length)\n",
    "        \n",
    "        train_embeddings = np.hstack([title_embeddings_train, description_embeddings_train])\n",
    "        test_embeddings = np.hstack([title_embeddings_test, description_embeddings_test])\n",
    "        for nn_config in nn_configs:\n",
    "            train_model(\n",
    "                train_embeddings,\n",
    "                train_data[\"success\"],\n",
    "                test_embeddings,\n",
    "                test_data[\"success\"],\n",
    "                hidden_sizes=nn_config[\"hidden_sizes\"],\n",
    "                model_name=model_name,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
